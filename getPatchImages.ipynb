{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "source": [
    "# step2: divided into patches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### train, val_for_training(only nodefect) 이미지들을 패치로 쪼개서 tensor형태로 저장\n",
    "\n",
    "- test, validation set은 test/validation 함수 내에서 patch화 할것이므로 전처리 단계에서는 제외한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 저장되어있는 모든 crop된 이미지들 불러오기(grayscale로) - images list에 저장\n",
    "train_images = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in glob.glob(\"./data/nocrop/train/*.png\")]\n",
    "print(train_images[0].shape)\n",
    "\n",
    "val_for_training_images = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in glob.glob(\"./data/nocrop/val_for_training/*.png\")]\n",
    "print(val_for_training_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_images)) # 85개\n",
    "print(train_images[0].shape) # 256,2096\n",
    "\n",
    "print(len(val_for_training_images)) # 28개\n",
    "print(val_for_training_images[0].shape) # 256,2096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "patch로 쪼개주는 함수\n",
    ": image list를 받아서, tensor로 변환 \n",
    "-> .view(패치개수, 패치사이즈, 패치사이즈)\n",
    "-> train_patches.shape = 85 256 64 64 / val_for_training_patches.shape = 28 256 64 64\n",
    "\"\"\"\n",
    "def patch(images):\n",
    "  patch_size = 64\n",
    "  for i in range(len(images)):\n",
    "    images[i] = transforms.ToTensor()((images[i]))\n",
    "    # unfold로 shape조절 --> view --> [patch개수, patch_size, patch_size]\n",
    "    patches = images[i].data.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size) \n",
    "    patches = patches.contiguous().view(-1, patch_size, patch_size) \n",
    "    #print(patches.shape)\n",
    "    if i == 0:\n",
    "      patch = patches\n",
    "    if i > 0:\n",
    "      patches = torch.cat((patch, patches))\n",
    "      patch = patches\n",
    "  # print(patches.shape)\n",
    "\n",
    "  patches = patches.view(len(images), -1, patch_size, patch_size) \n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "patch를 이미지로 gdrive에 저장해주는 함수\n",
    ": patches텐서를 이미지번호 폴더에, 각 256개의 패치를 저장\n",
    "\"\"\"\n",
    "def save_patches(patches, set):\n",
    "  import os\n",
    "  patch_path=\"./data/nocrop/patch\"\n",
    "  if not os.path.isdir( patch_path ) :\n",
    "    os.mkdir( patch_path )\n",
    "\n",
    "  # cv2.imwrite사용하기 위해 patches를 numpy로 변환\n",
    "  patches = patches.numpy()\n",
    "\n",
    "  for i in range(1, patches.shape[0]+1): # original image 1 ~ 85번(폴더구분)\n",
    "    for num in range(1, patches.shape[1]+1): # patch 정보는 파일이름에 저장\n",
    "          path = os.path.join(\"./data/nocrop/patch\", set, \"image{0:02d}\".format(i))\n",
    "          if not os.path.isdir( path ) :\n",
    "            os.mkdir( path )\n",
    "          filename = \"{0:03d}.png\".format(num)\n",
    "          print(path + \"/\" + filename)\n",
    "          # image로 저장해서 다시 로드해오려고함\n",
    "          # 이미지로저장하기위해 다시 *255\n",
    "          cv2.imwrite(path + \"/\" + filename, patches[i-1,num-1,:]*255.0) \n",
    "  print(\"Saving pathces complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches = patch(train_images)\n",
    "print(train_patches.shape) # 85 256 64 64 # 사이즈=64*64;이미지당 패치개수=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_for_training_patches = patch(val_for_training_images)\n",
    "print(val_for_training_patches.shape) # 28 256 64 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_patches(train_patches, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_patches(val_for_training_patches, \"val_for_training\")"
   ]
  }
 ]
}